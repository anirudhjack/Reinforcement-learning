{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EE16BTECH11013_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqOBR68zySXC",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\textbf{Question 1:}\\\\\n",
        "\\textbf{Markov process}\n",
        "$$\n",
        "$\n",
        "Given\\;that\\\\\n",
        "TT + (TS\\;or\\;TT)---->TT\\;with\\;probability=1/2,(TS\\;or\\;ST)\\;with\\;probability=1/2.\\\\\n",
        "(a)\\implies Possible\\;states\\;are\\;TT,TS\\rightarrow TT,TS\\;or\\;ST.\\\\\n",
        "SS,TS\\rightarrow SS,ST\\;or\\;TS.\\\\\n",
        "TS,TS\\rightarrow\\;TT,TS\\;or\\;ST,SS.\\\\\n",
        "\\implies Transition\\;matrix\\;is\\left(\\begin{array}{cc} \n",
        "P_{TT-TT} & P_{TT-TS} & P_{TT-SS}\\\\\n",
        "P_{TS-TT} & P_{TS-TS} & P_{TS-SS}\\\\\n",
        "P_{SS-TT} & P_{SS-TS} & P_{SS-SS}\n",
        "\\end{array}\\right)\n",
        "=\\left(\\begin{array}{cc} \n",
        "1/2 & 1/2 & 0\\\\\n",
        "1/4 & 1/2 & 1/4\\\\\n",
        "0 & 1/2 & 1/2\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "$\n",
        "(b)We\\;start\\;with\\;a\\;medium\\;height\\;individual\\;(as\\;the\\;first\\;partner).\n",
        "\\implies Second\\;generation\\;transition\\;matrix=(Transition\\;matrix)^{2}=\n",
        "\\left(\\begin{array}{cc} \n",
        "1/2 & 1/2 & 0\\\\\n",
        "1/4 & 1/2 & 1/4\\\\\n",
        "0 & 1/2 & 1/2\n",
        "\\end{array}\\right)\n",
        "*\\left(\\begin{array}{cc} \n",
        "1/2 & 1/2 & 0\\\\\n",
        "1/4 & 1/2 & 1/4\\\\\n",
        "0 & 1/2 & 1/2\n",
        "\\end{array}\\right)\n",
        "=(1/4)^{2}\\left(\\begin{array}{cc} \n",
        "6 & 8 & 2\\\\\n",
        "4 & 8 & 4\\\\\n",
        "2 & 8 & 6\n",
        "\\end{array}\\right)\\\\\n",
        "\\implies Third\\;generation\\;transition\\;matrix=(Transition\\;matrix)^{3}=\n",
        "(1/4)^{2}\\left(\\begin{array}{cc} \n",
        "6 & 8 & 2\\\\\n",
        "4 & 8 & 4\\\\\n",
        "2 & 8 & 6\n",
        "\\end{array}\\right)\n",
        "*\\left(\\begin{array}{cc} \n",
        "1/2 & 1/2 & 0\\\\\n",
        "1/4 & 1/2 & 1/4\\\\\n",
        "0 & 1/2 & 1/2\n",
        "\\end{array}\\right)\n",
        "=(1/4)^{3}\\left(\\begin{array}{cc} \n",
        "20 & 32 & 12\\\\\n",
        "16 & 32 & 16\\\\\n",
        "12 & 32 & 20\n",
        "\\end{array}\\right)\\\\\n",
        "\\implies The\\;probabilities\\;that\\;any\\;offspring;belonging\\;to\\;first\\;generation\\;to\\;be\\;tall\\;or\\;medium\\;or\\;short=\n",
        "\\left(\\begin{array}{cc}\n",
        "1/4 &1/2 &1/4\n",
        "\\end{array}\\right)\\\\\n",
        "\\implies The\\;probabilities\\;that\\;any\\;offspring;belonging\\;to\\;second\\;generation\\;to\\;be\\;tall,\\;medium\\;and\\;short=(1/4)^{2}\n",
        "\\left(\\begin{array}{cc}\n",
        "4 &8 &4\n",
        "\\end{array}\\right)\n",
        "=\\left(\\begin{array}{cc}\n",
        "1/4 &1/2 &1/4\n",
        "\\end{array}\\right)\\\\\n",
        "\\implies The\\;probabilities\\;that\\;any\\;offspring;belonging\\;to\\;third\\;generation\\;to\\;be\\;tall,\\;medium\\;and\\;short=(1/4)^{3}\n",
        "\\left(\\begin{array}{cc}\n",
        "16 &32 &16\n",
        "\\end{array}\\right)\n",
        "=\\left(\\begin{array}{cc}\n",
        "1/4 &1/2 &1/4\n",
        "\\end{array}\\right)\\\\\n",
        "$\n",
        "$\n",
        "(c)The\\;probabilities\\;that\\;any\\;offspring;belonging\\;to\\;n^{th}\\;generation\\;to\\;be\\;tall,\\;medium\\;and\\;short=(1/4)^{n}\n",
        "\\left(\\begin{array}{cc}\n",
        "2^{2n-2} &2^{2n-1} &2^{2n-2}\n",
        "\\end{array}\\right)\n",
        "=\\left(\\begin{array}{cc}\n",
        "1/4 &1/2 &1/4\n",
        "\\end{array}\\right)\\\\\n",
        "$\n",
        "$$\n",
        "\\blacksquare\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHX4TkELCQcK",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\textbf{Question 2}\\\\\n",
        "\\textbf{Markov Reward Process}\n",
        "$$\n",
        "$\n",
        "(a)The\\;states\\;of\\;markov\\;process\\;are\\;S,1,2,3,4,5,6,7,8,9,W.\\\\\n",
        "\\;\\;\\;The\\;transition\\;matrix\\;for\\;this\\;game\\;is=\\left(\\begin{array}{cc} \n",
        "P_{SS} & P_{S1} & P_{S2}&....&P_{SW}\\\\\n",
        "P_{1S} & P_{11} & P_{12}&....&P_{1W}\\\\\n",
        ". &.&.&....\\\\\n",
        ". &.&.&....\\\\\n",
        "P_{WS} &P_{W1}&P_{W2}&....&P_{WW}\n",
        "\\end{array}\\right)\n",
        "=\n",
        "\\left(\\begin{array}{cc}\n",
        "0&1/4&1/4&1/4&1/4&0&0&0&0&0&0\\\\\n",
        "0&0&1/4&1/4&1/4&1/4&0&0&0&0&0\\\\\n",
        "0&0&0&0&0&0&0&1&0&0&0\\\\\n",
        "0&0&0&0&1/4&1/4&1/4&1/4&0&0&0\\\\\n",
        "0&0&0&0&0&0&0&0&1&0&0\\\\\n",
        "0&0&0&0&0&0&1/4&1/4&1/4&1/4&0\\\\\n",
        "0&0&0&0&0&0&0&1/4&1/4&1/4&1/4\\\\\n",
        "0&0&0&0&0&0&0&1/4&1/4&1/4&1/4\\\\\n",
        "0&0&0&0&0&0&0&0&1/2&1/4&1/4\\\\\n",
        "0&0&0&1&0&0&0&0&0&0&0\\\\\n",
        "0&0&0&0&0&0&0&0&0&0&1\\\\\n",
        "\\end{array}\\right)\n",
        "$\n",
        "$\n",
        "(b)There\\;is\\;one\\;absorbing\\;state:StateW.\\\\\n",
        "\\implies Reward(R(s(t)))=\\left(\\begin{array}{cc}\n",
        "1&1&0&1&0&1&1&1&1&0&0\n",
        "\\end{array}\\right)\n",
        "^{T}\\\\\n",
        "\\implies Discount\\;factor(\\gamma)=1\\\\\n",
        "\\implies Average\\;no\\;of\\;steps\\;equals\\;to\\;value\\;function\\;at\\;each\\;state.\\\\\n",
        "\\implies From\\;Bell\\;man\\;equation\\;V=(1-\\gamma.N)^{-1}.R\\;\\;where\\;N\\;is\\;fundamental\\;matrix.\\\\\n",
        "\\implies P=\\left(\\begin{array}{cc}\n",
        "N & R\\\\\n",
        "0 & A\n",
        "\\end{array}\\right)\\;where\\;A\\;is\\;an\\;identity\\;matrix\\;with\\;absorbing\\;states.\\\\\n",
        "\\implies Here\\;Fundamental\\;matrix(N)=\\left(\\begin{array}{cc}\n",
        "0&1/4&1/4&1/4&1/4&0&0&0&0&0\\\\\n",
        "0&0&1/4&1/4&1/4&1/4&0&0&0&0\\\\\n",
        "0&0&0&0&0&0&0&1&0&0\\\\\n",
        "0&0&0&0&1/4&1/4&1/4&1/4&0&0\\\\\n",
        "0&0&0&0&0&0&0&0&1&0\\\\\n",
        "0&0&0&0&0&0&1/4&1/4&1/4&1/4\\\\\n",
        "0&0&0&0&0&0&0&1/4&1/4&1/4\\\\\n",
        "0&0&0&0&0&0&0&1/4&1/4&1/4\\\\\n",
        "0&0&0&0&0&0&0&0&1/2&1/4\\\\\n",
        "0&0&0&1&0&0&0&0&0&0\\\\\n",
        "\\end{array}\\right)\\\\\n",
        "\\implies We\\;have\\;to\\;remove\\;absorbing\\;states\\;form\\;transition\\;matrix\\;to\\;form\\;fundamental\\;matrix.\\\\\n",
        "\\implies V=(1-\\gamma.N)^{-1}.R\\\\\n",
        "\\implies V=\\left(\\begin{array}{cc}\n",
        "85/12&7&16/3&20/3&16/3&20/3&16/3&16/3&16/3&20/3\n",
        "\\end{array}\\right)\n",
        "^{T}\\\\\n",
        "\\implies V=\\left(\\begin{array}{cc}\n",
        "7.08&7&5.33&6.67&5.33&6.67&5.33&5.33&5.33&6.67\n",
        "\\end{array}\\right)\n",
        "^{T}\\\\\n",
        "$\n",
        "\n",
        "$$\n",
        "\\blacksquare\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ4c1dgfkr_E",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\textbf{Question 3:}\\\\\n",
        "\\textbf{Markov Decision Process}\n",
        "$$\n",
        "![](https://drive.google.com/uc?id=1Fvs_A4qVhuFPanP1t8hnM463XU3t4zQ3)\n",
        "\n",
        "$\n",
        "(b)\\implies Deterministic\\;policy:\\\\\\pi(s)=\\left\\{ \\begin{array}{ll} Drive(a=1), & if\\;S=Top,Rolling,Bottom \\\\\n",
        " \\end{array} \\right..\n",
        " \\\\\\implies Stocastic\\;policy\\;at\\;any\\;state:\\\\\\pi(s)=\\left\\{ \\begin{array}{ll} Drive(a=1)\\;with\\; probability(p)=0.9 \\\\Not\\;Drive(a=0)\\;with\\;probability(p)=0.1\n",
        " \\\\\n",
        " \\end{array} \\right..\n",
        "$\n",
        "\n",
        "$\n",
        "(c)Transition\\;matrix\\;for\\;Deterministic\\;policy:\\\\\n",
        "P_{Drive}=\\left(\\begin{array}{cc}\n",
        "0.8 & 0.2 &0\\\\\n",
        "0.3 & 0.4 &0.3\\\\\n",
        "0.7&0.3&0\n",
        "\\end{array}\\right)\\\\\n",
        "$\n",
        "![](https://drive.google.com/uc?id=1l95WhodfTNbfPYp3U_ryz-JHRMFY7o3s)\n",
        "$\n",
        "Transition\\;matrix\\;for\\;Stocastic\\;policy:\\\\\n",
        "\\implies \\pi(a=1/s)=0.9,\\pi(a=0/s)=0.1\\;\\forall\\;s\\;in\\;S\\\\\n",
        "P_{Drive}=\\left(\\begin{array}{cc}\n",
        "0.8 & 0.2 &0\\\\\n",
        "0.3 & 0.4 &0.3\\\\\n",
        "0.7&0.3&0\n",
        "\\end{array}\\right)\\\\\n",
        "$\n",
        "![](https://drive.google.com/uc?id=1FjdJueS7438aDLDeB5zcXgdJncxEtmpt)\n",
        "$\n",
        "P_{NotDrive}=\\left(\\begin{array}{cc}\n",
        "0.6 & 0.4 &0\\\\\n",
        "0 & 0.1 &0.9\\\\\n",
        "0&0&1\n",
        "\\end{array}\\right)\\\\\n",
        "$\n",
        "![](https://drive.google.com/uc?id=1VEgSR2zMOIXT_4tbkbi7wvHoz3582R8L)\n",
        "$\n",
        "(d)History\\;dependent\\;policy:\\rightarrow History\\;dependent\\;policy\\;depends\\;on\\;previous\\;states\\\\\n",
        "Time\\;step\\;1:a_{1}^{hd}(drive)=drive\\\\\n",
        "a_{1}^{hd}(rolling\\;down)=drive\\\\\n",
        "a_{1}^{hd}(bottom)=drive\\\\\n",
        "\\rightarrow Time\\;step\\;2:a_{2}^{hd}(drive,s_{t-1}=top)=Not\\;drive\\\\\n",
        "a_{2}^{hd}(drive,s_{t-1}=rolling down)=drive\\\\\n",
        "a_{2}^{hd}(drive,s_{t-1}=bottom)=drive\\\\\n",
        "$\n",
        "\n",
        "$$\n",
        "\\blacksquare\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRSHCBXFsInJ",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\textbf{Question 4:}\\\\\n",
        "\\textbf{Policy Evaluation and Partial Ordering of Policies}\n",
        "$$\n",
        "$\n",
        "Given:Stocastic\\;Environment\\implies 90 \\% of\\;the\\;times\\;and\\;fails\\;10\\;\\% \\;of\\;the\\;times.\\\\\n",
        "The\\;state\\;D\\;is\\;a\\;terminal\\;state\\;with\\;reward\\;of\\;100.\\\\\n",
        "$\n",
        "$\n",
        "(a)V^{\\pi}(s)\\;for\\;each\\;policy:\\\\\n",
        "Policy\\;1:Policy\\;π1\\;is\\;deterministic\\;policy\\;that\\;chooses\\;action\\;a1\\;at\\; all\\;states\\;s ∈ S.\\\\\n",
        "\\implies We\\;have\\;V^{\\pi}(s)=\\sum_{a}\\pi(a/s)\\sum_{s^{'}}P^{a}_{ss^{'}}(R^{a}_{ss^{'}}+\\gamma.V^{\\pi}(s^{'}))\\\\\n",
        "\\implies V_{\\pi}(A)=(0.9)(-10+1.V^{\\pi}(B))+(0.1)(-10+1.V^{\\pi}(C))\\\\\n",
        "\\implies V_{\\pi}(B)=(0.9)(-10+1.V^{\\pi}(D))+(0.1)(-10+1.V^{\\pi}(A))\\\\\n",
        "\\implies V_{\\pi}(C)=(0.9)(-10+1.V^{\\pi}(A))+(0.1)(-10+1.V^{\\pi}(D))\\\\\n",
        "\\implies V_{\\pi}(D)=100\\\\\n",
        "\\implies V^{\\pi}(A)=75.6,V^{\\pi}(B)=87.5,V^{\\pi}(C)=68.04\\\\\n",
        "Policy\\;2:Policy\\;π2\\;is\\;deterministic\\;policy\\;that\\;chooses\\;action\\;a2\\;at\\; all\\;states\\;s ∈ S.\\\\\n",
        "\\implies We\\;have\\;V^{\\pi}(s)=\\sum_{a}\\pi(a/s)\\sum_{s^{'}}P^{a}_{ss^{'}}(R^{a}_{ss^{'}}+\\gamma.V^{\\pi}(s^{'}))\\\\\n",
        "\\implies V_{\\pi}(A)=(0.9)(-10+1.V^{\\pi}(C))+(0.1)(-10+1.V^{\\pi}(B))\\\\\n",
        "\\implies V_{\\pi}(B)=(0.9)(-10+1.V^{\\pi}(A))+(0.1)(-10+1.V^{\\pi}(D))\\\\\n",
        "\\implies V_{\\pi}(C)=(0.9)(-10+1.V^{\\pi}(D))+(0.1)(-10+1.V^{\\pi}(A))\\\\\n",
        "\\implies V_{\\pi}(D)=100\\\\\n",
        "\\implies V^{\\pi}(A)=75.6,V^{\\pi}(B)=68.04,V^{\\pi}(C)=87.5\\\\\n",
        "Policy\\;3:Policy\\;π3\\;is\\;a\\;stochastic\\;policy\\\\\n",
        "\\implies We\\;have\\;V^{\\pi}(s)=\\sum_{a}\\pi(a/s)\\sum_{s^{'}}P^{a}_{ss^{'}}(R^{a}_{ss^{'}}+\\gamma.V^{\\pi}(s^{'}))\\\\\n",
        "\\implies V_{\\pi}(A)=(0.6)(0.1)(-10+1.V^{\\pi}(B))+(0.4)(0.1)(-10+1.V^{\\pi}(C))+\n",
        "(0.4)(0.9)(-10+1.V^{\\pi}(B))+(0.9)(0.6)(-10+1.V^{\\pi}(C))=(0.58)(-10+1.V^{\\pi}(C))+(0.42)(-10+1.V^{\\pi}(B)).\\\\\n",
        "\\implies V_{\\pi}(B)=(0.9)(-10+1.V^{\\pi}(A))+(0.1)(-10+1.V^{\\pi}(D)).\\\\\n",
        "\\implies V_{\\pi}(C)=(0.9)(-10+1.V^{\\pi}(D))+(0.1)(-10+1.V^{\\pi}(A)).\\\\\n",
        "\\implies V_{\\pi}(D)=100\\\\\n",
        "\\implies V^{\\pi}(A)=77.7778,V^{\\pi}(B)=87.77778,V^{\\pi}(C)=87.77778\\\\\n",
        "$\n",
        "$\n",
        "(b)Best\\;policy\\;among\\;the\\;policies:We\\;can\\;clearly\\;observe\\;that\\;\\pi_{3}\\;is\\;best\\;policy\\;since\\;V^{\\pi_{3}}(A)>V^{\\pi_{1}}(A),V^{\\pi_{3}}(B)>V^{\\pi_{1}}(B),V^{\\pi_{3}}(C)>V^{\\pi_{1}}(C)\\;and\\;V^{\\pi_{3}}(A)>V^{\\pi_{2}}(A),V^{\\pi_{3}}(B)>V^{\\pi_{2}}(B),V^{\\pi_{3}}(C)>V^{\\pi_{2}}(C).\n",
        "$\n",
        "\n",
        "$\n",
        "(c)Two\\;policies\\;are\\;comparable\\;\\implies V^{\\pi_{a}}(s)>V^{\\pi_{b}}(s)\\;\\forall\n",
        "s ∈ S\\\\\n",
        "\\implies V^{\\pi_{1}},V^{\\pi_{2}}\\;are\\;not\\;comparable\\;since\\;V^{\\pi_{1}}(B)>V^{\\pi_{2}}(B)\\;and\\;V^{\\pi_{1}}(C)<V^{\\pi_{2}}(C)\n",
        "$\n",
        "$$\n",
        "\\blacksquare\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxcmtqXxB2Ts",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\textbf{Question 5:}\\\\\n",
        "\\textbf{ Optimal Policies}\n",
        "$$\n",
        "$\n",
        "Given:A\\;policy\\;π^{*}\\;of\\;an\\;MDP\\;is\\;said\\;to\\;be\\;optimal\\;if\\;π^{*}\\;∈\\;Π\\;\\\\ where\\;Π = arg max_{π}(V_{π}(s)) = arg max_{\\pi}(E_{π} (rt+1 + γrt+1 + · · · |st = s))\\\\\n",
        "\\implies We\\;can\\;find\\;optimal\\;policy\\;through\\;policy\\;iteration.\\\\\n",
        "\\implies \\textbf{Policy iteration algorithm:}\\\\\n",
        "Start\\;with\\;any\\;initial\\;policy\\;\\pi_{1}.\\\\\n",
        "\\;\\;\\textbf{for}\\;i=1,2,......,N\\;\\textbf{do}:\\\\\n",
        "\\;\\;\\;\\;\\;Evaluate\\;V^{\\pi_{i}}(s)\\;\\forall\\;s\\in S.That\\;is\\\\\n",
        "\\;\\;\\;\\;\\; for\\;k = 1,2,··· ,K\\;\\textbf{do}:\\\\\n",
        "\\;\\;\\;\\;\\;\\;\\;\\;For\\;all\\;s ∈S\\;calculate: \\\\\n",
        "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;V^{\\pi_{i}}_{k+1}(s)\\leftarrow\\sum_{a}\\pi(a/s)\\sum_{s^{'}}P^{a}_{ss^{'}}(R^{a}_{ss^{'}}+\\gamma.V^{\\pi_{i}}_{k}(s^{'}))\\\\\n",
        "\\\\\n",
        "\\;\\;\\;\\;\\;\\;\\textbf{end for}\\\\\n",
        "\\;\\;\\;\\;\\;\\;Perform\\;policy\\;Improvement\\\\\n",
        "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\pi_{i+1} = greedy(V_{\\pi_{i}} )\\\\\n",
        "\\textbf{end for}\\\\\n",
        "\\implies\\;Proof\\;that\\;it\\;attains\\;optimal\\;policy\\\\\n",
        "\\rightarrow Consider\\;a\\;Deterministic\\;policy\\;a=\\pi(s)\n",
        "\\rightarrow \\pi^{'}(s)=argmax_{a}(Q^{\\pi}(s,a))\\\\\n",
        "\\rightarrow This\\;improves\\;the\\;value\\;from\\;any\\;state\\;s\\;over\\;one\\;step.\\\\\n",
        "Q^{\\pi}(s,\\pi^{'}(s))=max_{a}(Q^{\\pi}(s,a))\\geq Q^{\\pi}(s,\\pi(s))=V^{\\pi}(s).\\\\\n",
        "\\implies V^{pi^{'}(s)}\\geq V_{\\pi}(s)\n",
        "\\implies It\\;therefore\\;improves\\;the\\;value\\;function.\\\\\n",
        "\\rightarrow If\\;improvement\\;stops:\\\\\n",
        "Q^{\\pi}(s,\\pi^{'}(s))=max_{a}(Q^{\\pi}(s,a))= Q^{\\pi}(s,\\pi(s))=V^{\\pi}(s).\\\\\n",
        "\\implies Bellman\\;optimalityequation\\;is\\;satisfied.\\\\\n",
        "V^{\\pi}(s)=max_{a}(Q^{\\pi}(s,a))\\\\\n",
        "\\implies The\\;policy\\;\\pi\\;for\\;which\\;the\\;improvement\\;stops\\;is\\;optimal\\;\\\\V^{\\pi}(s)=V_{*}(s)\\;\\forall \\;s\\in S.\n",
        "$\n",
        "$$\n",
        "\\blacksquare\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjmRUaX_Q5Zj",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\textbf{Question 6:}\\\\\n",
        "\\textbf{Value Iteration}\n",
        "$$\n",
        "$\n",
        "(a)\\textbf{Optimal policy:}\\\\The\\;optimal\\;policy\\;is\\;to\\;move\\;right,i.e\\;\\pi(s)={R}\\;\\forall s\\in {s_{1},s_{2},s_{3},s_{4},s_{5}}.\\\\\n",
        "\\textbf{Optimal value function:}\\\\\n",
        "\\implies We\\;have\\;V^{\\pi}(s)=\\sum_{a}\\pi(a/s)\\sum_{s^{'}}P^{a}_{ss^{'}}(R^{a}_{ss^{'}}+\\gamma.V^{\\pi}(s^{'}))\\\\\n",
        "\\implies V^{\\pi}(s_{1})=0+(1).(V^{\\pi}(s_{2}))\\\\\n",
        "\\implies V^{\\pi}(s_{2})=0+(1).(V^{\\pi}(s_{3}))\\\\\n",
        "\\implies V^{\\pi}(s_{3})=0+(1).(V^{\\pi}(s_{4}))\\\\\n",
        "\\implies V^{\\pi}(s_{4})=0+(1).(V^{\\pi}(s_{5}))\\\\\n",
        "\\implies V^{\\pi}(s_{5})=0+(1).(V^{\\pi}(s_{6}))\\\\\n",
        "\\implies terminal\\;state(V^{\\pi}(s_{6}))=10\\\\\n",
        "\\implies V^{\\pi}(s_{1})=V^{\\pi}(s_{2})=V^{\\pi}(s_{3})=V^{\\pi}(s_{4})=V^{\\pi}(s_{5})=10\n",
        "$\n",
        "$\n",
        "(b)Given\\;\\gamma={0.9, 0.5, 0.1}.\\\\\n",
        "\\textbf{Value function for different discount factors:}\\\\\n",
        "\\implies V^{\\pi}(s_{1})=0+(\\gamma).(V^{\\pi}(s_{2}))\\\\\n",
        "\\implies V^{\\pi}(s_{2})=0+(\\gamma).(V^{\\pi}(s_{3}))\\\\\n",
        "\\implies V^{\\pi}(s_{3})=0+(\\gamma).(V^{\\pi}(s_{4}))\\\\\n",
        "\\implies V^{\\pi}(s_{4})=0+(\\gamma).(V^{\\pi}(s_{5}))\\\\\n",
        "\\implies V^{\\pi}(s_{5})=0+(\\gamma).(V^{\\pi}(s_{6}))\\\\\n",
        "\\implies terminal\\;state(V^{\\pi}(s_{6}))=10\\\\\n",
        "\\implies V^{\\pi}(s_{1})=10\\gamma ^{5},V^{\\pi}(s_{2})=10\\gamma ^{4},V^{\\pi}(s_{3})=10\\gamma ^{3},V^{\\pi}(s_{4})=10\\gamma ^{2},V^{\\pi}(s_{5})=10\\gamma ^{1}.\\\\ \n",
        "\\implies We\\;can\\;see\\;with\\;new\\;gamma\\;the\\;policy\\;doesn't\\;change.\\;\\\\\n",
        "$\n",
        "$\n",
        "(c)\\textbf{Adding a constant c for the rewards of all the states:}\\\\\n",
        "\\implies The\\;optimal\\;policy\\;is\\;to\\;move\\;right.\n",
        "\\implies V^{\\pi}(s_{1})=c+(\\gamma).(V^{\\pi}(s_{2}))\\\\\n",
        "\\implies V^{\\pi}(s_{2})=c+(\\gamma).(V^{\\pi}(s_{3}))\\\\\n",
        "\\implies V^{\\pi}(s_{3})=c+(\\gamma).(V^{\\pi}(s_{4}))\\\\\n",
        "\\implies V^{\\pi}(s_{4})=c+(\\gamma).(V^{\\pi}(s_{5}))\\\\\n",
        "\\implies V^{\\pi}(s_{5})=c+(\\gamma).(V^{\\pi}(s_{6}))\\\\\n",
        "\\implies terminal\\;state(V^{\\pi}(s_{6}))=10+c\\\\\n",
        "\\implies V^{\\pi}(s_{1})=c(1+\\gamma+...+\\gamma^{4})+10.\\gamma^{5}\\\\\n",
        "\\implies V^{\\pi}(s_{2})=c(1+\\gamma+...+\\gamma^{3})+10.\\gamma^{4}\\\\\n",
        "\\implies V^{\\pi}(s_{3})=c(1+\\gamma+\\gamma^{2})+10.\\gamma^{3}\\\\\n",
        "\\implies V^{\\pi}(s_{4})=c(1+\\gamma)+10.\\gamma^{2}\\\\\n",
        "\\implies V^{\\pi}(s_{5})=c+10.\\gamma\\\\\n",
        "\\implies V^{\\pi}(s_{6})=10+c\\\\\n",
        "\\implies The\\;optimal\\;policy\\;doesn't\\;change\\;we\\;see\\;it\\;through\\;value\\;iteration.\n",
        "$\n",
        "\n",
        "$\n",
        "(d)Relation\\;between \\;V^{\\pi}\\;and\\;\\tilde V^{\\pi}:\\\\\n",
        "\\implies we\\;have\\;V^{\\pi}(s)=E_{\\pi}(G_{t}/S_{t}=s)\\\\\n",
        "\\implies V^{\\pi}(s)=E_{\\pi}(\\sum_{k=0}^{\\infty}\\gamma^{k}(R_{t+k+1}/s_{t}=s))\\\\\n",
        "\\implies we\\;have\\;\\tilde V^{\\pi}(s)=E_{\\pi}(\\tilde G_{t}/S_{t}=s)\\\\\n",
        "\\implies Given\\;that\\;\\tilde G_{t}=G_{t}+c\\\\\n",
        "\\implies \\tilde V^{\\pi}(s)=E_{\\pi}(\\sum_{k=0}^{\\infty}\\gamma^{k}(R_{t+k+1}+c/s_{t}=s))\\\\\n",
        "\\implies \\tilde V^{\\pi}(s)=E_{\\pi}(\\sum_{k=0}^{\\infty}\\gamma^{k}(R_{t+k+1}/s_{t}=s))+c.E_{\\pi}(\\sum_{k=0}^{\\infty}\\gamma^{k}/s_{k}=s)\\\\\n",
        "\\implies \\tilde V^{\\pi}(s)=V^{\\pi}(s)+\\frac{c}{1-\\gamma}\n",
        "\\implies We\\;can\\;see\\;the\\;relative\\;values\\;of\\;rewards\\;doesn't\\;changed\\;by\\;adding\\;a\\;constant.\\\\\n",
        "\\implies The\\;policy\\;doesn't\\;change\\;it\\;remains\\;going\\;to\\;right\n",
        "$\n",
        "$$\n",
        "\\blacksquare\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf-tR8KnvwNc",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\textbf{Question 7:}\\\\\n",
        "\\textbf{ Effect of Noise and Discounting}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1br1JxbN632",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "obstacles=[(1,1),(2,1),(2,3)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7RRQvVd9mpP",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "Function\\;for\\;improving\\;value\\;functions\\;greedily\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Soah7Auu6-w7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy(i,j,noise,gamma,Vinitial):\n",
        "  up,down,left,right=0,0,0,0\n",
        "  if i>0 and ((i-1,j) not in obstacles):\n",
        "    up=gamma*Vinitial[i-1][j]\n",
        "  elif i==0 or ((i-1,j) in obstacles):\n",
        "    up=gamma*Vinitial[i][j]\n",
        "  if i<4 and ((i+1,j) not in obstacles):\n",
        "    down=gamma*Vinitial[i+1][j]\n",
        "  elif i==4 or ((i+1,j) in obstacles):\n",
        "    down=gamma*Vinitial[i][j]\n",
        "  if j>0 and ((i,j-1) not in obstacles):\n",
        "    left=gamma*Vinitial[i][j-1]\n",
        "  elif j==0 or ((i,j-1) in obstacles):\n",
        "    left=gamma*Vinitial[i][j]\n",
        "  if j<4 and ((i,j+1) not in obstacles):\n",
        "    right=gamma*Vinitial[i][j+1]\n",
        "  elif j==4 or ((i,j+1) in obstacles):\n",
        "    right=gamma*Vinitial[i][j]\n",
        "  total_reward=up+down+right+left\n",
        "  up_reward=(1-noise)*up+(noise/3)*(total_reward-up)\n",
        "  down_reward=(1-noise)*down+(noise/3)*(total_reward-down)\n",
        "  left_reward=(1-noise)*left+(noise/3)*(total_reward-left)\n",
        "  right_reward=(1-noise)*right+(noise/3)*(total_reward-right)\n",
        "  return max(up_reward,down_reward,left_reward,right_reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALdjNF-2_sJK",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "Policy\\;iteration\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDVtY6md_xcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Policy_iteration(iterations,noise,gamma,Vinitial):\n",
        "  for k in range(iterations):\n",
        "    for i in range(np.shape(Vinitial)[0]):\n",
        "      for j in range(np.shape(Vinitial)[1]):\n",
        "        if (i,j) in obstacles or(i==2 and j==2)or(i==2 and j==np.shape(Vinitial)[1]-1)or(i==np.shape(Vinitial)[0]-1):\n",
        "          continue\n",
        "        else :\n",
        "          Vinitial[i][j]=greedy(i,j,noise,gamma,Vinitial)\n",
        "  return Vinitial"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-8mrDtxNMU1",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "Policy\\;iteration\\;with\\;\\gamma=0.1\\;and\\;noise=0.5\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iObM36WGBlGd",
        "colab_type": "code",
        "outputId": "87722ac9-cae9-4906-bde4-5bfea9e4e944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "V1=[[0,0,0,0,0]\n",
        "    ,[0,0,0,0,0]\n",
        "    ,[0,0,1,0,10]\n",
        "    ,[0,0,0,0,0]\n",
        "    ,[-10,-10,-10,-10,-10]]\n",
        "\n",
        "V1=Policy_iteration(iterations=4000\n",
        "                   ,noise=0.5\n",
        "                  ,gamma=0.1\n",
        "                  ,Vinitial=V1)\n",
        "print(\"Value Function matrix after Policy Iteration\")\n",
        "print(np.array(V1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value Function matrix after Policy Iteration\n",
            "[[ 6.20061487e-06  1.36896486e-04  2.64459852e-03  1.85474875e-03\n",
            "   2.63790105e-02]\n",
            " [-5.10537939e-05  0.00000000e+00  5.13465557e-02  2.68021892e-02\n",
            "   5.09375953e-01]\n",
            " [-2.97972189e-03  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
            "   1.00000000e+01]\n",
            " [-1.72670708e-01 -1.78632633e-01 -1.22218206e-01 -1.54459711e-01\n",
            "   3.36365090e-01]\n",
            " [-1.00000000e+01 -1.00000000e+01 -1.00000000e+01 -1.00000000e+01\n",
            "  -1.00000000e+01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grJPNEhjXu0D",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\textbf{Observations}\n",
        "$$\n",
        "$\n",
        "\\rightarrow Prefer\\;the\\;close\\;exit\\;(state\\;with\\;reward\\;+1)\\;by\\;avoiding\\;the\\;cliff\\; (solid\\;path\\;to\\;+1)\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04L9UgpKSvnK",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "Value\\;iteration\\;with\\;\\gamma=0.1\\;and\\;noise=0\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH1cw5I0S9BW",
        "colab_type": "code",
        "outputId": "39a24836-4ca7-44c2-c985-d98ebeec4d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "V2=[[0,0,0,0,0]\n",
        "    ,[0,0,0,0,0]\n",
        "    ,[0,0,1,0,10]\n",
        "    ,[0,0,0,0,0]\n",
        "    ,[-10,-10,-10,-10,-10]]\n",
        "\n",
        "V2=Policy_iteration(iterations=4000\n",
        "                   ,noise=0\n",
        "                  ,gamma=0.1,\n",
        "                  Vinitial=V2)\n",
        "print(\"Value Function matrix after Policy Iteration\")\n",
        "print(np.array(V2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value Function matrix after Policy Iteration\n",
            "[[ 1.e-04  1.e-03  1.e-02  1.e-02  1.e-01]\n",
            " [ 1.e-05  0.e+00  1.e-01  1.e-01  1.e+00]\n",
            " [ 1.e-04  0.e+00  1.e+00  0.e+00  1.e+01]\n",
            " [ 1.e-03  1.e-02  1.e-01  1.e-01  1.e+00]\n",
            " [-1.e+01 -1.e+01 -1.e+01 -1.e+01 -1.e+01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgjtpmbzY81-",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\textbf{Observations}\n",
        "$$\n",
        "$\n",
        "\\rightarrow \\;Prefer\\;the\\;close\\;exit\\;(state\\;with\\;reward\\;+1)\\;but\\;risk\\;the\\;cliff (dashed\\;path\\;to\\;+1)\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKMLtTVbTi-3",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "Value\\;iteration\\;with\\;\\gamma=0.9\\;and\\;noise=0\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN8FZuHfTeUn",
        "colab_type": "code",
        "outputId": "082acb86-1b74-4900-ba10-248d49bbe056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "V3=[[0,0,0,0,0]\n",
        "    ,[0,0,0,0,0]\n",
        "    ,[0,0,1,0,10]\n",
        "    ,[0,0,0,0,0]\n",
        "    ,[-10,-10,-10,-10,-10]]\n",
        "\n",
        "V3=Value_iteration(iterations=4000\n",
        "                   ,noise=0\n",
        "                  ,gamma=0.9,\n",
        "                  Vinitial=V3)\n",
        "print(\"Value Function matrix after Policy Iteration\")\n",
        "print(np.array(V3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value Function matrix after Policy Iteration\n",
            "[[  5.31441    5.9049     6.561      7.29       8.1     ]\n",
            " [  4.782969   0.         7.29       8.1        9.      ]\n",
            " [  5.31441    0.         1.         0.        10.      ]\n",
            " [  5.9049     6.561      7.29       8.1        9.      ]\n",
            " [-10.       -10.       -10.       -10.       -10.      ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNv2c0I3ZqMw",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\textbf{Observations}\n",
        "$$\n",
        "$\n",
        "\\rightarrow Prefer\\;the\\;distant\\;exit\\;(state\\;with\\;reward\\;+10)\\;but\\;risk\\;the\\;cliff\\;(dashed\\; path\\; to\\;+10).\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXJLXMcrUKBP",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "Value\\;iteration\\;with\\;gamma=0.9\\;and\\;noise=0.5\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SORVF4aDUTda",
        "colab_type": "code",
        "outputId": "17e0ddfb-9c24-449d-fca7-cdcc71bc02b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "V4=[[0,0,0,0,0]\n",
        "    ,[0,0,0,0,0]\n",
        "    ,[0,0,1,0,10]\n",
        "    ,[0,0,0,0,0]\n",
        "    ,[-10,-10,-10,-10,-10]]\n",
        "\n",
        "V4=Value_iteration(iterations=4000\n",
        "                   ,noise=0.5\n",
        "                  ,gamma=0.9,\n",
        "                  Vinitial=V4)\n",
        "print(\"Value Function matrix after Policy Iteration\")\n",
        "print(np.array(V4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value Function matrix after Policy Iteration\n",
            "[[  2.01684288   2.67225336   3.4845576    4.50288017   5.59761815]\n",
            " [  1.39517334   0.           3.56493254   5.23890892   7.20644595]\n",
            " [  0.46028028   0.           1.           0.          10.        ]\n",
            " [ -2.03754538  -2.9269313   -1.51613289  -0.18062129   3.49753742]\n",
            " [-10.         -10.         -10.         -10.         -10.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TxAU522adE-",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "Observations\n",
        "$$\n",
        "$\n",
        "\\rightarrow Prefer\\;the\\;distant\\;exit\\;(state\\;with\\;reward\\;+10)\\;by\\;avoiding\\;the\\;cliff\\;(solid\\; path\\;to\\;+10).\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVDiCG1tflPo",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\textbf{Question 8:}\\\\\n",
        "\\textbf{Convergence Rate of Value Iteration}\n",
        "$$\n",
        "$\n",
        "We\\;have\\;V_{k+1}(s)\\leftarrow max[\\sum_{s^{'}\\in S}P^{a}_{ss^{'}}(R^{a}_{ss^{'}}+\\gamma V_{k}(s^{'}))]\\\\\n",
        "\\rightarrow Since\\;environment\\;is\\;deterministic:\\\\\n",
        "V_{k+1}(s)\\leftarrow max[\\sum_{s^{'}\\in S}(R^{a}_{ss^{'}}+\\gamma\\sum_{s^{'}\\in S}P^{a}_{ss^{'}}.V_{k}(s^{'}))]\\\\\n",
        "\\implies we\\;have\\;Bellman\\;operator\\rightarrow L(v)=max_{a\\in A}(R^{a}+\\gamma.P^{a}V)\\\\\n",
        "\\implies L(v_{k})=V_{k+1}\\\\\n",
        "\\implies for\\;convergence\\;Lt_{k\\rightarrow \\infty}L(V_{k})=V^{*}.\\\\\n",
        "\\implies Inorder\\;to\\;prove\\;value\\;iteration\\;converges\\;we\\;have\\;to\\;show\\;that\\;||V_{k+1}-V*||_{\\infty}\\;converges.\\\\\n",
        "\\rightarrow Consider\\;||V_{k+1}-V*||_{\\infty}=||L(V_{k})-L(V*)||_{\\infty}\\leq\\gamma||V_{k}-V*||_{\\infty}\\\\\n",
        "\\rightarrow we\\;have\\;\\gamma||V_{k}-V*||_{\\infty}=\\gamma||L(V_{k-1})-L(V*)||_{\\infty}\\\\\n",
        "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\leq \\gamma^{2}||V_{k-1}-V*||_{\\infty}\\\\\n",
        "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\leq \\gamma^{3}||V_{k-2}-V*||_{\\infty}\\\\\n",
        "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;..\\\\\n",
        "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;..\\\\\n",
        "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;..\\\\\n",
        "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\leq \\gamma^{k}||V_{1}-V*||_{\\infty}\\\\\n",
        "\\implies The\\;term\\;\\gamma^{k}||V_{1}-V*||_{\\infty}converges\\;goemetrically\\;to\\;zero.i.e\\;||V_{k+1}-V*||_{\\infty}\\rightarrow 0.\\\\\n",
        "\\therefore Value\\;iteration\\;converges\\;geometrically.\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2NFzf_JvqBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}